{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group-3 - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, must use Python 3!\n",
    "\n",
    "Functions will scrape listing data and sold_date from webpages specified. \n",
    "\n",
    "Removed data for sold properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Import libraries\n",
    "##################################################\n",
    "import pandas as pd\n",
    "import re\n",
    "import ssl\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from urllib.request import Request, urlopen\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#################################################\n",
    "# function defined to extract miscellaneous attributes from each listing\n",
    "# - get the values \n",
    "# - clean the data\n",
    "# - create a datalist\n",
    "#################################################\n",
    "\n",
    "def extract_misc_information(all_misc_information):\n",
    "    misc_data_dictionary = {}\n",
    "    # Iterating through the all misc information scraped from the web-page\n",
    "    # and generate a data dictionary for all values extracted\n",
    "    for values in all_misc_information:\n",
    "        misc_list_as_key_value=list(filter(str.strip,values.get_text(separator=':').strip().split(':')))\n",
    "        if(len(misc_list_as_key_value) == 2):\n",
    "            key = misc_list_as_key_value[0]\n",
    "            value = misc_list_as_key_value[1]\n",
    "            misc_data_dictionary[key]=value\n",
    "    \n",
    "    # Data-key for attributes to be retrieved from the miscellaneous data-dictionary\n",
    "    misc_keys_as_list = ['CONSTRUCTION STATUS','LAUNDRY','HOA AMENITIES','Appliances included','Tax assessed value','Annual tax amount','HOA AMENITIES','POOL','HOA Fees Freq','STYLE','List Date','WATER/SEWER','Class','View description','LOT DESCRIPTION']\n",
    "\n",
    "    # Iterating through the data-key and retrieveing values from miscllaneous data-dictionary\n",
    "    # If value does-not exists then adding 'None'\n",
    "    misc_data_list = []\n",
    "    for misc_key in misc_keys_as_list:\n",
    "        if misc_key in misc_data_dictionary:\n",
    "            misc_data_list.append(misc_data_dictionary.get(misc_key).replace('$','').replace(',',''))\n",
    "        else:\n",
    "            misc_data_list.append('None')\n",
    "    return misc_data_list\n",
    "\n",
    "#################################################\n",
    "# function defined to extract zestimate information from each listing\n",
    "# - get the values \n",
    "# - clean the data\n",
    "#################################################\n",
    "\n",
    "def extract_zestimate_information(zestimate_information):\n",
    "    zestimate_list = []\n",
    "    \n",
    "    # Extracting zestimate information here\n",
    "    # Index 1: Zestimate value\n",
    "    # Index 2: Min - Max Zestimate range.\n",
    "    # Index 3: Last 30 day price increase\\decrease\n",
    "    # Index 4: Next 1 year price increase\\decrease\n",
    "    index=0\n",
    "    for values in zestimate_information:\n",
    "        index=index+1\n",
    "        if(index == 1) :\n",
    "            zestimate_value = int(values.get_text().replace('$','').replace(',',''))\n",
    "            zestimate_list.append(zestimate_value)\n",
    "        elif (index == 2) :\n",
    "            range = values.get_text().split(' - ')\n",
    "            minimum_value = range[0].replace('$','')\n",
    "            if(\"M\" in minimum_value):\n",
    "                minimum_value = float(minimum_value.replace('M',''))*1000000\n",
    "            elif(isinstance(minimum_value, (float,str))):\n",
    "                minimum_value = float(minimum_value.replace(',',''))\n",
    "            else :\n",
    "                minimum_value = float(-1)\n",
    "                \n",
    "            maximum_value = range[1].replace('$','')\n",
    "            if(\"M\" in maximum_value):\n",
    "                maximum_value = float(maximum_value.replace('M',''))*1000000\n",
    "            elif(isinstance(maximum_value, (float,str))):\n",
    "                maximum_value = float(maximum_value.replace(',',''))\n",
    "            else :\n",
    "                maximum_value = float(-1)\n",
    "            zestimate_list.append(minimum_value)\n",
    "            zestimate_list.append(maximum_value)\n",
    "        elif(index == 3) :\n",
    "            #Data -> (Ex.+$25,421 (+2.4 %))\n",
    "            price_change_last_month = values.get_text().split(' ')\n",
    "            if(\"$\" in price_change_last_month[0]):\n",
    "                change_money_last_month=int(price_change_last_month[0].replace('$','').replace(',',''))   \n",
    "            else:\n",
    "                change_money_last_month = int(-1)\n",
    "            \n",
    "            if(\".\" in price_change_last_month[1]):\n",
    "                change_percent_last_month = (price_change_last_month[1].replace('(','').replace('\\u200a','').replace('%','').replace(')',''))\n",
    "            else:\n",
    "                change_percent_last_month=float(-1)\n",
    "\n",
    "            zestimate_list.append(change_money_last_month)\n",
    "            zestimate_list.append(change_percent_last_month)\n",
    "        elif(index == 4) :\n",
    "            #$1,078,596 (-1.2 %)\n",
    "            price_change_next_year = values.get_text().split(' ')\n",
    "            \n",
    "            if((\"$\" in price_change_next_year[0])):\n",
    "                change_money_next_year = (price_change_next_year[0].replace('$','').replace(',',''))\n",
    "            else:\n",
    "                change_money_next_year = int(-1)\n",
    "               \n",
    "            \n",
    "            if(\".\" in price_change_next_year[1]):\n",
    "                change_percent_next_year = (price_change_next_year[1].replace('(','').replace('\\u200a','').replace('%','').replace(')',''))\n",
    "            else:\n",
    "                change_percent_next_year = float(-1)\n",
    "            \n",
    "            zestimate_list.append(change_money_next_year)\n",
    "            zestimate_list.append(change_percent_next_year)\n",
    "\n",
    "    # Filling up empty\\missing data with 'None'\n",
    "    while(len(zestimate_list)<7):\n",
    "        zestimate_list.append('None')\n",
    "    \n",
    "    return zestimate_list\n",
    "\n",
    "#################################################\n",
    "# function defined to extract facts and features from each listing\n",
    "# - get the values \n",
    "# - clean the data\n",
    "#################################################\n",
    "\n",
    "def extract_facts_and_features(facts_and_features):\n",
    "    data_list = []\n",
    "    index = 0\n",
    "    # Extracting first five entries of facts and fetures\n",
    "    # [Type,Year_Built,Heating,Cooling,Parking]\n",
    "    for values in facts_and_features:\n",
    "        index = index + 1\n",
    "        if(index < 6) :\n",
    "            data_list.append(values.get_text())\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "#################################################\n",
    "# function defined to extract school ratings from each listing\n",
    "# - get the values \n",
    "# - clean the data\n",
    "#################################################\n",
    "def extract_school_ratings(school_ratings):\n",
    "    school_information_list = []\n",
    "    # Iterating through school rating and filling up list with relevant information\n",
    "    for values in school_ratings:\n",
    "        school_information_list.append(values.get_text())\n",
    "    # Filling up empty\\missing data with 'None'\n",
    "    while(len(school_information_list)<3):\n",
    "        school_information_list.append('None')\n",
    "    return school_information_list\n",
    "\n",
    "#################################################\n",
    "# function defined to extract school distance from each listing\n",
    "# - get the values \n",
    "# - clean the data\n",
    "#################################################\n",
    "\n",
    "def extract_school_distance(schoolDistance):\n",
    "    school_information_list = []\n",
    "\n",
    "    # Iterating through school distance and filling up list with relevant information\n",
    "    # Using index to pull only first three distances\n",
    "    # The data here has first school - name and then distance format hence skipping first \n",
    "    # entry always\n",
    "    index = 0\n",
    "    for values in schoolDistance:\n",
    "        index = index + 1\n",
    "        if(index%2 == 0 and index < 7):\n",
    "            school_information_list.append(values.get_text().replace(' mi',''))\n",
    "\n",
    "    # Filling up empty\\missing data with 'None'\n",
    "    while(len(school_information_list)<3):\n",
    "        school_information_list.append('None')\n",
    "    return school_information_list\n",
    "\n",
    "\n",
    "#################################################\n",
    "# function defined to extract data from each url listing on the main page of zillow \n",
    "# Note:\n",
    "# While looping through each url from the main zillow page, \n",
    "# each url had its own response time for returning the data. \n",
    "# Due to which irregular data was fetched.\n",
    "# To over come this issue we created a loop which will keep hitting the url till the data is not returned \n",
    "# and then move on to the next url\n",
    "\n",
    "#################################################\n",
    "\n",
    "def pull_drill_down_data(url,trial_number):\n",
    "    #print(url)\n",
    "    # Making the website believe that you are accessing it using a Mozilla Firefox web browser\n",
    "    req = Request(url, headers = {'user-Agent':'Mozilla/5.0'})\n",
    "    webpage = urlopen(req)\n",
    "    webpageData=webpage.read()\n",
    "    \n",
    "    # Creating a BeautifulSoup object of the html page for easy extraction of data\n",
    "    soup = BS(webpageData, 'html.parser')\n",
    "    \n",
    "    # Find all tags with some given name and attributes\n",
    "    fact_and_deatures = soup.findAll('span',attrs={'class':'ds-body ds-home-fact-value'})\n",
    "    zestimate_information =  soup.findAll('div',attrs={'class':'content'})\n",
    "    school_ratings = soup.findAll('span',attrs={'class':'ds-schools-display-rating'})\n",
    "    school_distance = soup.findAll('span',attrs={'ds-school-value ds-body-small'})\n",
    "    all_misc_information = soup.findAll('span',attrs={'class':'Text-aiai24-0 IJYzV'})\n",
    "    \n",
    "    \n",
    "    # The service sometimes does-not respond back with proper data.\n",
    "    # Making a recursive call here in those situations. \n",
    "    # If after 50 tries the data is not received then putting the code to\n",
    "    # sleep for about 15 seconds befor trying to retrive data again\n",
    "    final_data_list = []\n",
    "    if(trial_number < 50 and (len(fact_and_deatures) == 0 or len(zestimate_information) == 0 or len(school_ratings) == 0 or len(school_distance) == 0 or len(all_misc_information) == 0)):\n",
    "        #print(trial_number)\n",
    "        if(trial_number%50 == 0):\n",
    "            time.sleep(15)\n",
    "        return pull_drill_down_data(url, trial_number+1)\n",
    "    else:\n",
    "        final_data_list = [*extract_facts_and_features(fact_and_deatures),*extract_zestimate_information(zestimate_information),*extract_school_ratings(school_ratings),*extract_school_distance(school_distance),*extract_misc_information(all_misc_information)]\n",
    "        #print(final_data_list)\n",
    "        return list(final_data_list)\n",
    "\n",
    "#################################################\n",
    "# function defined to extract data from the main zillow search page\n",
    "# - get the values \n",
    "# - clean the data\n",
    "#################################################\n",
    "def pull_data(url):\n",
    "    #print(url)\n",
    "    # For ignoring SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    # Making the website believe that you are accessing it using a Mozilla Firefox web browser\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'})\n",
    "    webpage = urlopen(req)\n",
    "    webPageData = webpage.read()\n",
    "    # Creating a BeautifulSoup object of the html page for easy extraction of data\n",
    "    soup = BS(webPageData,'html.parser')\n",
    "\n",
    "    # View how the tags are nested in the document\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    # Find all tags with some given name and attributes\n",
    "    listing_details = soup.findAll(\"div\", {\"class\": \"list-card-info\"})\n",
    "    listing_drilldown_url = soup.findAll('a',attrs = {'class':'list-card-link', 'tabindex':\"0\"}, href = True)\n",
    " \n",
    "\n",
    "    # Retrieve the contents of a tag (list-card-info)\n",
    "    listing_details_list = []\n",
    "    for tag in listing_details:\n",
    "        listing_details_list.append(tag.get_text(separator='/').strip().split('/'))\n",
    "    #print(listing_details_list)\n",
    "    \n",
    "    # Retrieve links\n",
    "    listing_links = []\n",
    "    for div in listing_details:\n",
    "        links = div.findAll('a')\n",
    "        for a in links:\n",
    "            listing_links.append(a['href'])\n",
    "\n",
    "    # clean up above code\n",
    "    listing_details_list_cleaned = []\n",
    "    for i in listing_details_list:\n",
    "        listing_details_list_cleaned.append(list(filter(str.strip,i)))\n",
    "    \n",
    "    listing_details_from_drilldown_cleaned = []\n",
    "    for url in listing_drilldown_url:\n",
    "        time.sleep(5)\n",
    "        listing_details_from_drilldown_cleaned.append(pull_drill_down_data(url['href'],1))\n",
    "    \n",
    "    # Combining all data together\n",
    "    data = [[*i, j,*k] for i,j,k in zip(listing_details_list_cleaned, listing_links,listing_details_from_drilldown_cleaned)]\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "##################################################\n",
    "def grab_webpages(num_pages, url):\n",
    "    url_list = [] # create empty list for multiple pages\n",
    "\n",
    "    # pull out first 4 pages of data\n",
    "    for num in range(5, num_pages):\n",
    "        text_to_replace = \"{%22currentPage%22:1}\"\n",
    "        new_url = re.sub(text_to_replace, \"{%22currentPage%22:\"+str(num)+\"}\", url)\n",
    "        url_list.append(new_url)\n",
    "\n",
    "    # scrape data and join together\n",
    "    empty_list = []  \n",
    "    for url_page in url_list:\n",
    "        empty_list.append(pull_data(url_page))\n",
    "\n",
    "    # sum list of lists\n",
    "    result = sum(empty_list, [])\n",
    "\n",
    "    # create a dataframe of result\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    return df\n",
    "\n",
    "##################################################\n",
    "# call function\n",
    "##################################################\n",
    "# prepare dataframe\n",
    "df_sold_listings = grab_webpages(19, \"https://www.zillow.com/homes/recently_sold/house_type/1-_beds/3_p/?searchQueryState={%22pagination%22:{%22currentPage%22:1},%22usersSearchTerm%22:%22Bay%20Area,%20CA%22,%22mapBounds%22:{%22west%22:-122.9904853197909,%22east%22:-121.71332467525966,%22south%22:37.330427377062136,%22north%22:38.088750071545604},%22isMapVisible%22:true,%22filterState%22:{%22beds%22:{%22min%22:1},%22isForSaleByAgent%22:{%22value%22:false},%22isForSaleByOwner%22:{%22value%22:false},%22isNewConstruction%22:{%22value%22:false},%22isForSaleForeclosure%22:{%22value%22:false},%22isComingSoon%22:{%22value%22:false},%22isAuction%22:{%22value%22:false},%22isPreMarketForeclosure%22:{%22value%22:false},%22isPreMarketPreForeclosure%22:{%22value%22:false},%22isRecentlySold%22:{%22value%22:true},%22isCondo%22:{%22value%22:false},%22isMultiFamily%22:{%22value%22:false},%22isManufactured%22:{%22value%22:false},%22isLotLand%22:{%22value%22:false},%22isTownhouse%22:{%22value%22:false},%22isApartment%22:{%22value%22:false}},%22isListVisible%22:true}\")\n",
    "\n",
    "##################################################\n",
    "# data cleanup and writing to relevant output files\n",
    "##################################################\n",
    "# drop useless columns in df\n",
    "df_sold_listings = df_sold_listings.drop([1, 4, 6, 8], axis=1)\n",
    "\n",
    "# add appropriate column headers\n",
    "df_sold_listings.columns = ['address', 'price', 'bds', 'ba', 'sqft', 'link','type','year_built','heating','cooling','parking','zestimate','zestimate-Min','zestimate-Max','price-variation-last-30-days','percent-price-variation-last-30-days','price-forecast-one-year','percent-price-forecast-one-year','elementary-school-rating','junior-high-school-rating','high-school-rating','elementary-school-distance','junior-high-school-distance','high-school-distance','construction-status','laundry','hoa-ameneties','appliances-included','tax-assessed-value','annual-tax-amount','hoa-amenities','pool','hoa-fees-frequency','style','list-data','water-sewer','class','view-description','lot-description']\n",
    "\n",
    "# clean up data for analysis\n",
    "df_sold_listings['price'] = df_sold_listings['price'].str.replace('$', '')\n",
    "df_sold_listings['price'] = df_sold_listings['price'].str.replace(',', '')\n",
    "df_sold_listings['price'] = df_sold_listings['price'].str.replace('M', '*1e6').map(pd.eval).astype(int)\n",
    "df_sold_listings['sqft'] = df_sold_listings['sqft'].str.replace(',', '')\n",
    "\n",
    "# split address column\n",
    "temp_list = [df_sold_listings['address'].str.split(',', expand=True), df_sold_listings]\n",
    "temp_table = pd.concat(temp_list, axis=1)\n",
    "temp_table.columns = ['street', 'city', 'zip_code', 'address', 'price', 'bds', 'ba', 'sqft', 'links','type','year_built','heating','cooling','parking','zestimate','zestimate-Min','zestimate-Max','price-variation-last-30-days','percent-price-variation-last-30-days','price-forecast-one-year','percent-price-forecast-one-year','elementary-school-rating','junior-high-school-rating','high-school-rating','elementary-school-distance','junior-high-school-distance','high-school-distance','construction-status','laundry','hoa-ameneties','appliances-included','tax-assessed-value','annual-tax-amount','hoa-amenities','pool','hoa-fees-frequency','style','list-data','water-sewer','class','view-description','lot-description']\n",
    "temp_table.columns = temp_table.columns.str.strip()\n",
    "df_sold_listings = temp_table\n",
    "\n",
    "# change datatype\n",
    "cols = ['price', 'sqft']\n",
    "df_sold_listings[cols] = df_sold_listings[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# export to excel\n",
    "##################################################\n",
    "# display data\n",
    "display(df_sold_listings)\n",
    "fileName=\"output_zillow.xlsx\"\n",
    "df_sold_listings.to_excel(fileName,sheet_name='Sheet_name_1')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
